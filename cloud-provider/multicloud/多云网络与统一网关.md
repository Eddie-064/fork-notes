# 多云网络 - MultiCloud Networking Software - MCNS

几乎所有的云厂商都直接提供了自家的 L4/L7 层负载均衡服务，而且提供的 API 各有区别。此外市场上也有若干
开源或闭源的 L4/L7 层负载均衡。

那么是用开源组件自建负载均衡合适，还是应该直接使用云厂商提供的负载均衡服务，还是选择第三方厂商的负载
均衡服务？如果考虑到迁移到多云的可能性，又该如何选择负载均衡服务呢？还是说我们应该更进一步，直接使用
VPN 打通多个云服务商的内网（自建或者使用第三方服务）？

本文的主要目的就是回答这些问题。

最理想的情况，当然是一套多云服务组件，能够完全屏蔽掉底层云服务商之间的区别，使我们的应用原生跨云，可
以很方便地跨云管理、跨云调度。但是理想的场景不一定符合当前的实际状况。

## DNS 负载均衡

许多 DNS 服务商都提供按地域的 DNS 就近解析、DNS 加权负载均衡等功能，比如 Route53，可以使用这些在 DNS
层实现多 IP 之间的负载均衡。

## L4 层负载均衡

L4 层负载均衡的配置参数很少，可替换性很高。

在公有云上，我目前认为最佳方案是直接使用各云服务商提供的 L4 层负载均衡服务（如 AWS NLB），没必要自建
或者使用第三方服务。

但是某些情况下，开源方案相比自建方案确实会有些优势，比如新特性或者更低的成本，因此也可以看看如下开源
方案：

- https://github.com/acassen/keepalived
  - 基于 VRRP 实现 VIP 的高可用，基于 IPVS 实现 L4 负载均衡
  - 是最常用的自建 L4 负载均衡方案，在云上自行搭建 keepalived 也是有案例可询的，值得考虑。
- https://github.com/facebookincubator/katran
  - Facebook 开源的一个基于 BPF 跟 XDP 的高性能 L4 负载均衡
- https://www.haproxy.com/blog/haproxy-on-aws-best-practices-part-3/
  - haproxy 算是比较传统的 L4 负载均衡
- https://github.com/iqiyi/dpvs
  - 爱奇艺开源的基于 DPDK 与 LVS 的 L4 负载均衡，但是项目不活跃了

在国外的云服务上设计网络架构时，需要重点考虑的一点是——**跨区流量成本**。在四层负载均衡中，如果设计为
在所有 upstream 上进行负载均衡，肯定会产生响应比例的跨区流量成本。而如果设计为仅在各可用区内分别进行
负载均衡，那就还需要在**每个可用区内单独做高可用**（K8s 的强项），并且还得具备（在部分可用区故障
时）**屏蔽部分可用区的能力**（直接在 L4 负载均衡上关闭掉某个区，DNS 中去掉该区 LB 的解析）。

而对于国内的云服务，可以忽略掉跨区流量成本，只考虑高可用的能力。

## L7 层负载均衡 - API 网关

为了避免被云服务商绑定或者为了在多云上拥有统一的网关层使用体验，比较建议自建 L7 负载均衡。

自建 L7 负载均衡的部署方案有两种，一是前端直接使用云服务商的 L4 负载均衡作为代理，相关文档：

- [Active-Active HA for NGINX Plus on AWS Using AWS Network Load Balancer](https://docs.nginx.com/nginx/deployment-guides/amazon-web-services/high-availability-network-load-balancer/)
  - Nginx 官方提供的在 AWS 上使用 AWS NLB + Nginx 的部署方案

另一个方案就是跟裸机部署一样，使用 keedalived 做高可用，相关文档：

- [Active-Passive HA for NGINX Plus on AWS Using Elastic IP Addresses](https://docs.nginx.com/nginx/deployment-guides/amazon-web-services/high-availability-keepalived/)
  - Nginx 官方也提供的在 AWS 上使用 keepalived + Nginx 的部署方案

你可以在 Nginx 官方文档中看到其他云服务商的部署方案。

### API 网关选型

参考 [/kubernetes/ingress-egress/README.md](/kubernetes/ingress-egress/README.md)

### Kubernetes 多集群网关架构

#### 1. Istio Multi-cluster

对于多集群位于同一网络的情况，集群的 Pods 间可以直接通讯，这正是我们想要的。对于跨云场景，最好也是直
接使用专线把网络打通多云的内部网络，使之可以互通。因此这里我们只讨论多集群位于同一网络内的情况，暂时
忽略掉多集群网络不互通的场景。

我主要关注如下几点：

- 控制面与网关层如何做到高可用？
  - 控制面跟网关单独用两个集群部署？这样即使挂了一个控制面集群，另一个仍然能工作。而且这样成本也不
    高，就多几个 istiod 的 pods 而已。
- 如何选型多集群网关？
  - Istio 提供的 IngressGateway 估计不太够用，可能需要基于 EnvoyFilter 甚至 WASM 做一些二次开发。
  - 更好的选择可能是选用 APISIX/Kong/Gloo 等第三方网关，对于非 Envoy 方案，再直接注入一个 Sidecar
    Proxy 使网关能无缝接入服务网格中，依托 Istio 来支持多集群的请求处理。
- 多集群的 Istio 升级时要如何处理？
- 能否实现将服务间的通讯限制在同一可用区避免跨区流量？
- 如何实现为服务任选多个集群进行部署？
  - 使用 karmada 感觉是个比较好的方式

## 多云网关架构

在单云内，网络是互通的，因此总体来看只需要一个 L7 边缘网关就能处理所有的流量。

而多云场景下，如果多云之间是完全等价的，那就只需要在 DNS 上完成多云之间的负载均衡就行。

但是通常而言，所云之间并不完全等价，这时可能就需要一个多云之间的「全局 L7 网关」，负责处理流量在多云
之间的分配。

但是实际上并不需要如此，多集群的支持完全可以由 Istio 服务网格提供，而 L7 网关只需要专注于网关层的逻
辑就行。由服务网格的 Sidecar 处理多集群、多云相关的逻辑。目前感觉这是比较优的方案。

更进一步，在多集群与多云环境下，可以考虑使用单独的集群部署 L7 网关，业务服务部署在业务集群，再使用
Istio 服务网格打通这些集群。这样得到的仍然是一个单层的 L7 网关，但是网关都部署在单独的集群。Solo 提
供了一个比较清晰的示意图：

![](_img/gloo-mesh-multi-cluster.svg)

![](_img/gloo-multi-cluster-api-gateway.webp)

## 多云网络市场分析

针对多云网络市场，目前也有多家厂商在这个领域竞争，当前（2022-04）主要有如下几家（按字母顺序排列）：

- [Alkira Cloud Networking](https://www.alkira.com/cloud-networking/)
  - Single and multi-cloud networking should be as simple as any other cloud service.
- [Arrcus Multi-Cloud Networking](https://arrcus.com/solutions/multi-cloud/)
  - onsisting of ArcEdge, as a secure data plane software and ArcOrchestrator dramatically shortens
    multi-cloud networking set up time from days to hours.
  - 使用 Terraform 自动化部署多云网络
- [Arista Data-Driven Cloud Networking](https://www.arista.com/en/solutions/cloud-networking)
  - an open approach to cloud networking around a single consistent software platform, the Arista
    EOS® network stack, and network data lake architecture (NetDL™), with the application of
    artificial intelligence and machine learning (AI/ML) to automation and security challenges.
- [Aviatrix Cloud Networking Platform](https://aviatrix.com/cloud-network-platform/#multi-cloud-architecture/)
  - brings multi-cloud networking, security, and operational visibility capabilities that go beyond
    what any cloud service provider offers. Aviatrix software leverages public cloud provider APIs
    to interact with and directly program native cloud networking constructs, abstracting the unique
    complexities of each cloud to form one network data plane.
  - 使用 Terraform/Ansible 自动化部署多云网络
- [Cohesive VNS3](https://www.cohesive.net/)
  - Encrypted Overlay Network
  - Whether using a single cloud service provider, multiple clouds, or hybrid cloud, the VNS3
    Network Platform provides the connectivity and security you need with better pricing, better
    support, and better visibility.
- [Cisco Cloud ACPI](https://www.cisco.com/c/en/us/solutions/data-center-virtualization/application-centric-infrastructure/cloud-aci.html)
  - Automated network connectivity, consistent policy management, and simplified ops for multicloud.
- [F5 Distributed Cloud Platform](https://www.f5.com/cloud/products/platform-overview)
  - delivers improved functionality, advanced security controls, and more simplified operations than
    native services from cloud providers.
- [Prosimo](https://prosimo.io/)
  - One platform that integrates cloud networking, performance, security, observability, driven by
    data
- [VMware Avi](https://avinetworks.com/)
  - The Multi-Cloud Application Services Platform
- [VMware NSX Cloud](https://www.vmware.com/products/nsx-cloud.html)
  - Deliver consistent networking and security for your applications running natively in public
    clouds with NSX Cloud.

其中有的产品会使用 overlay 网络完全屏蔽各云服务商底层的网络差异，但是代价是性能更差；也有的产品直接
编排云服务商原生的网络能力实现跨云。

MCNS 提供所有核心网络能力，底层能力如路由，高级能力如 L4/L7 服务（内容分发网络 CDN，ADC、防火墙），
以及其他关键的能力集成，比如 terraform/pulumi/ansible 等云环境配置管理与自动化运维工具。如下图所示：

![](_img/MCNS_Features.png)

驱动用户选择多云网络方案而不是直接使用各云服务商原生网络的原因在于，直接使用原生网络方案存在如下痛
点：

- 配置太过繁琐：仅仅为了做一点微小的改动，比如说改下 ACL 规则，我就需要打开不同云服务商的 28 个浏览
  器页面，并且做一堆的配置变更。
- 带宽限制：通过 VPN 直接连接多云，通常会受限于 VPN 的带宽
- 统一网络堆栈：希望能够在统一的面板上管理所有云上的网络资源
- 高级网络路由：大部分云服务商提供的专有网络 VPC，都不支持 BGP 协议
- 各云服务商的功能、设计思路不一致，导致多云环境的运维、故障排查困难。

其他多云网络方案：

- [多云管理与 CloudFlare](https://www.cloudflare.com/zh-cn/learning/cloud/what-is-multicloud-management/)
- [F5’s Multi-Cloud Networking Strategy and the Gartner 2021 Cool Vendors in Cloud Computing Report](https://www.f5.com/company/blog/multi-cloud-gartner-cool-vendor-cloud-computing-report)
- [Market Guide for Multicloud Networking Software](https://aviatrix.com/gartner-multicloud-networking-software-market-guide/)
  - [Multicloud Networking Software (MCNS)](https://blogs.gartner.com/andrew-lerner/2022/04/21/multicloud-networking-software-mcns/)

总的来说，多云网络目前还处于早期发展阶段，overlay 的网络彻底屏蔽了各云服务商的网络差异，而 underlay
网络方案的配置复杂性很难避免。目前还是先观望观望吧，不急于作出选择，毕竟我目前接触到的场景，还没复杂
到必须选择一个多云网络架构，当前的架构优化优化，缝缝补补又能顶三年 emmmm
